{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.layers import Flatten, Dense, Input, Dropout, Conv1D, BatchNormalization, MaxPooling1D, Flatten, LSTM, Bidirectional, Embedding, Concatenate, Reshape\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam, SGD, RMSprop, Adamax\n",
    "\n",
    "import ast\n",
    "\n",
    "# Hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../../../Data/labelled_training_data.csv')\n",
    "val_data = pd.read_csv('../../../Data/labelled_validation_data.csv')\n",
    "test_data = pd.read_csv('../../../Data/labelled_testing_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"processId\"] = train_data[\"processId\"].map(lambda x: 0 if x in [0, 1, 2] else 1)  # Map to OS/not OS\n",
    "train_data[\"parentProcessId\"] = train_data[\"parentProcessId\"].map(lambda x: 0 if x in [0, 1, 2] else 1)  # Map to OS/not OS\n",
    "train_data[\"userId\"] = train_data[\"userId\"].map(lambda x: 0 if x < 1000 else 1)  # Map to OS/not OS\n",
    "train_data[\"mountNamespace\"] = train_data[\"mountNamespace\"].map(lambda x: 0 if x == 4026531840 else 1)  # Map to mount access to mnt/ (all non-OS users) /elsewhere\n",
    "train_data[\"eventId\"] = train_data[\"eventId\"]  # Keep eventId values (requires knowing max value)\n",
    "train_data[\"returnValue\"] = train_data[\"returnValue\"].map(lambda x: 0 if x == 0 else (1 if x > 0 else 2))\n",
    "\n",
    "\n",
    "val_data[\"processId\"] = val_data[\"processId\"].map(lambda x: 0 if x in [0, 1, 2] else 1)  # Map to OS/not OS\n",
    "val_data[\"parentProcessId\"] = val_data[\"parentProcessId\"].map(lambda x: 0 if x in [0, 1, 2] else 1)  # Map to OS/not OS\n",
    "val_data[\"userId\"] = val_data[\"userId\"].map(lambda x: 0 if x < 1000 else 1)  # Map to OS/not OS\n",
    "val_data[\"mountNamespace\"] = val_data[\"mountNamespace\"].map(lambda x: 0 if x == 4026531840 else 1)  # Map to mount access to mnt/ (all non-OS users) /elsewhere\n",
    "val_data[\"eventId\"] = val_data[\"eventId\"]  # Keep eventId values (requires knowing max value)\n",
    "val_data[\"returnValue\"] = val_data[\"returnValue\"].map(lambda x: 0 if x == 0 else (1 if x > 0 else 2)) \n",
    "\n",
    "\n",
    "test_data[\"processId\"] = test_data[\"processId\"].map(lambda x: 0 if x in [0, 1, 2] else 1)  # Map to OS/not OS\n",
    "test_data[\"parentProcessId\"] = test_data[\"parentProcessId\"].map(lambda x: 0 if x in [0, 1, 2] else 1)  # Map to OS/not OS\n",
    "test_data[\"userId\"] = test_data[\"userId\"].map(lambda x: 0 if x < 1000 else 1)  # Map to OS/not OS\n",
    "test_data[\"mountNamespace\"] = test_data[\"mountNamespace\"].map(lambda x: 0 if x == 4026531840 else 1)  # Map to mount access to mnt/ (all non-OS users) /elsewhere\n",
    "test_data[\"eventId\"] = test_data[\"eventId\"]  # Keep eventId values (requires knowing max value)\n",
    "test_data[\"returnValue\"] = test_data[\"returnValue\"].map(lambda x: 0 if x == 0 else (1 if x > 0 else 2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>processId</th>\n",
       "      <th>threadId</th>\n",
       "      <th>parentProcessId</th>\n",
       "      <th>userId</th>\n",
       "      <th>mountNamespace</th>\n",
       "      <th>eventId</th>\n",
       "      <th>argsNum</th>\n",
       "      <th>returnValue</th>\n",
       "      <th>sus</th>\n",
       "      <th>evil</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>188967.000000</td>\n",
       "      <td>188967.000000</td>\n",
       "      <td>188967.000000</td>\n",
       "      <td>188967.000000</td>\n",
       "      <td>188967.000000</td>\n",
       "      <td>188967.000000</td>\n",
       "      <td>188967.000000</td>\n",
       "      <td>188967.000000</td>\n",
       "      <td>188967.000000</td>\n",
       "      <td>188967.000000</td>\n",
       "      <td>188967.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>455.070015</td>\n",
       "      <td>0.989046</td>\n",
       "      <td>7347.754089</td>\n",
       "      <td>0.950499</td>\n",
       "      <td>0.853329</td>\n",
       "      <td>0.001752</td>\n",
       "      <td>88.802198</td>\n",
       "      <td>2.894569</td>\n",
       "      <td>1.620675</td>\n",
       "      <td>0.907349</td>\n",
       "      <td>0.838411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>61.824198</td>\n",
       "      <td>0.104088</td>\n",
       "      <td>1108.656349</td>\n",
       "      <td>0.216912</td>\n",
       "      <td>0.353779</td>\n",
       "      <td>0.041816</td>\n",
       "      <td>199.137059</td>\n",
       "      <td>0.638079</td>\n",
       "      <td>0.745558</td>\n",
       "      <td>0.289944</td>\n",
       "      <td>0.368074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>129.050634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>461.577225</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7555.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>470.889349</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7555.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>487.116843</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7555.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>496.629091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7705.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1010.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           timestamp      processId       threadId  parentProcessId  \\\n",
       "count  188967.000000  188967.000000  188967.000000    188967.000000   \n",
       "mean      455.070015       0.989046    7347.754089         0.950499   \n",
       "std        61.824198       0.104088    1108.656349         0.216912   \n",
       "min       129.050634       0.000000       1.000000         0.000000   \n",
       "25%       461.577225       1.000000    7555.000000         1.000000   \n",
       "50%       470.889349       1.000000    7555.000000         1.000000   \n",
       "75%       487.116843       1.000000    7555.000000         1.000000   \n",
       "max       496.629091       1.000000    7705.000000         1.000000   \n",
       "\n",
       "              userId  mountNamespace        eventId        argsNum  \\\n",
       "count  188967.000000   188967.000000  188967.000000  188967.000000   \n",
       "mean        0.853329        0.001752      88.802198       2.894569   \n",
       "std         0.353779        0.041816     199.137059       0.638079   \n",
       "min         0.000000        0.000000       2.000000       0.000000   \n",
       "25%         1.000000        0.000000      42.000000       3.000000   \n",
       "50%         1.000000        0.000000      42.000000       3.000000   \n",
       "75%         1.000000        0.000000      42.000000       3.000000   \n",
       "max         1.000000        1.000000    1010.000000       5.000000   \n",
       "\n",
       "         returnValue            sus           evil  \n",
       "count  188967.000000  188967.000000  188967.000000  \n",
       "mean        1.620675       0.907349       0.838411  \n",
       "std         0.745558       0.289944       0.368074  \n",
       "min         0.000000       0.000000       0.000000  \n",
       "25%         2.000000       1.000000       1.000000  \n",
       "50%         2.000000       1.000000       1.000000  \n",
       "75%         2.000000       1.000000       1.000000  \n",
       "max         2.000000       1.000000       1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp          float64\n",
       "processId            int64\n",
       "threadId             int64\n",
       "parentProcessId      int64\n",
       "userId               int64\n",
       "mountNamespace       int64\n",
       "processName         object\n",
       "hostName            object\n",
       "eventId              int64\n",
       "eventName           object\n",
       "stackAddresses      object\n",
       "argsNum              int64\n",
       "returnValue          int64\n",
       "args                object\n",
       "sus                  int64\n",
       "evil                 int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Stackaddress**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15426"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_addresses_df = pd.concat([train_data['stackAddresses'], val_data['stackAddresses'], test_data['stackAddresses']], axis=0)\n",
    "len(stack_addresses_df.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert String to List**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[140159195621643, 140159192455417, 94656731598592]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert string to list\n",
    "train_data.stackAddresses = train_data.stackAddresses.apply(ast.literal_eval)\n",
    "val_data.stackAddresses = val_data.stackAddresses.apply(ast.literal_eval)\n",
    "test_data.stackAddresses = test_data.stackAddresses.apply(ast.literal_eval)\n",
    "train_data.stackAddresses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset - Max length of stack addresses: 20\n",
      "Validation dataset - Max length of stack addresses: 20\n",
      "Testing dataset - Max length of stack addresses: 20\n"
     ]
    }
   ],
   "source": [
    "train_data['stack_address_len']=train_data.stackAddresses.apply(len)\n",
    "val_data['stack_address_len']=val_data.stackAddresses.apply(len)\n",
    "test_data['stack_address_len']=test_data.stackAddresses.apply(len)\n",
    "print(f\"Training dataset - Max length of stack addresses: {max(train_data['stack_address_len'])}\")\n",
    "print(f\"Validation dataset - Max length of stack addresses: {max(val_data['stack_address_len'])}\")\n",
    "print(f\"Testing dataset - Max length of stack addresses: {max(test_data['stack_address_len'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stack_address_len\n",
       "0     181438\n",
       "2       3428\n",
       "1       1991\n",
       "3       1748\n",
       "4        154\n",
       "20        50\n",
       "6         36\n",
       "15        28\n",
       "14        23\n",
       "5         19\n",
       "7         14\n",
       "8         13\n",
       "17         6\n",
       "11         6\n",
       "10         4\n",
       "16         4\n",
       "12         3\n",
       "9          2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['stack_address_len'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(max(train_data['stack_address_len'])):\n",
    "    train_data[f\"stack_{i+1}\"]=\"\"\n",
    "    val_data[f\"stack_{i+1}\"]=\"\"\n",
    "    test_data[f\"stack_{i+1}\"]=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in train_data.iterrows():\n",
    "    list_stack = [float(elem) for elem in row['stackAddresses']]\n",
    "    for i, elem in enumerate(list_stack):\n",
    "        train_data.at[index, f'stack_{i+1}'] = elem\n",
    "\n",
    "for index, row in val_data.iterrows():\n",
    "    list_stack = [float(elem) for elem in row['stackAddresses']]\n",
    "    for i, elem in enumerate(list_stack):\n",
    "        val_data.at[index, f'stack_{i+1}'] = elem\n",
    "\n",
    "\n",
    "for index, row in test_data.iterrows():\n",
    "    list_stack = [float(elem) for elem in row['stackAddresses']]\n",
    "    for i, elem in enumerate(list_stack):\n",
    "        test_data.at[index, f'stack_{i+1}'] = elem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp                                                   129.050634\n",
       "processId                                                            1\n",
       "threadId                                                           382\n",
       "parentProcessId                                                      0\n",
       "userId                                                               0\n",
       "mountNamespace                                                       1\n",
       "processName                                            systemd-resolve\n",
       "hostName                                               ip-10-100-1-217\n",
       "eventId                                                             41\n",
       "eventName                                                       socket\n",
       "stackAddresses       [140159195621643, 140159192455417, 94656731598...\n",
       "argsNum                                                              3\n",
       "returnValue                                                          1\n",
       "args                 [{'name': 'domain', 'type': 'int', 'value': 'A...\n",
       "sus                                                                  0\n",
       "evil                                                                 0\n",
       "stack_address_len                                                    3\n",
       "stack_1                                              140159195621643.0\n",
       "stack_2                                              140159192455417.0\n",
       "stack_3                                               94656731598592.0\n",
       "stack_4                                                               \n",
       "stack_5                                                               \n",
       "stack_6                                                               \n",
       "stack_7                                                               \n",
       "stack_8                                                               \n",
       "stack_9                                                               \n",
       "stack_10                                                              \n",
       "stack_11                                                              \n",
       "stack_12                                                              \n",
       "stack_13                                                              \n",
       "stack_14                                                              \n",
       "stack_15                                                              \n",
       "stack_16                                                              \n",
       "stack_17                                                              \n",
       "stack_18                                                              \n",
       "stack_19                                                              \n",
       "stack_20                                                              \n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.loc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Args**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "argsNum\n",
       "3    148719\n",
       "4     15147\n",
       "2     12493\n",
       "1     11707\n",
       "5       708\n",
       "0       193\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['argsNum'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35177"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data['args'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[{'name': 'fd', 'type': 'int', 'value': 17}, {'name': 'statbuf', 'type': 'struct stat*', 'value': '0x7FFE8293A360'}]\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['args'][4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split args**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split the string column into list of dictionaries and create new columns\n",
    "def split_and_expand(row):\n",
    "    if pd.isna(row):\n",
    "        return pd.Series([None] * 15)\n",
    "\n",
    "    dicts = ast.literal_eval(row)\n",
    "    result = {'name_{}'.format(i+1): None for i in range(5)}\n",
    "    result.update({'type_{}'.format(i+1): None for i in range(5)})\n",
    "    result.update({'value_{}'.format(i+1): None for i in range(5)})\n",
    "\n",
    "    for i, d in enumerate(dicts):\n",
    "        if i >= 5:\n",
    "            break\n",
    "        result['name_{}'.format(i+1)] = d.get('name')\n",
    "        result['type_{}'.format(i+1)] = d.get('type')\n",
    "        result['value_{}'.format(i+1)] = d.get('value')\n",
    "\n",
    "    return pd.Series(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_split = train_data['args'].apply(split_and_expand)\n",
    "train_data = pd.concat([train_data, args_split], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_split = val_data['args'].apply(split_and_expand)\n",
    "val_data = pd.concat([val_data, args_split], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_split = test_data['args'].apply(split_and_expand)\n",
    "test_data = pd.concat([test_data, args_split], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data1 = train_data.sample(frac=0.4, ignore_index=True)\n",
    "\n",
    "X_num_train = train_data[['processId', 'threadId', 'parentProcessId', 'userId', 'mountNamespace', 'eventId', 'argsNum', 'returnValue', 'stack_address_len']]\n",
    "X_cat_train = train_data[['processName', 'hostName', 'eventName',\n",
    "                     'stack_1', 'stack_2', 'stack_3', 'stack_4',\n",
    "                     'stack_5', 'stack_6', 'stack_7', 'stack_8', 'stack_9', \n",
    "                     'stack_10','stack_11', 'stack_12', 'stack_13', 'stack_14', 'stack_15',\n",
    "                     'stack_16','stack_17', 'stack_18', 'stack_19', 'stack_20',\n",
    "                     'name_1', 'name_2', 'name_3', 'name_4', 'name_5',\n",
    "                     'type_1', 'type_2', 'type_3', 'type_4', 'type_5',\n",
    "                     'value_1', 'value_2', 'value_3', 'value_4', 'value_5']].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_num_val = val_data[['processId', 'threadId', 'parentProcessId', 'userId', 'mountNamespace', 'eventId', 'argsNum', 'returnValue', 'stack_address_len']]\n",
    "X_cat_val = val_data[['processName', 'hostName', 'eventName',\n",
    "                     'stack_1', 'stack_2', 'stack_3', 'stack_4',\n",
    "                     'stack_5', 'stack_6', 'stack_7', 'stack_8', 'stack_9', \n",
    "                     'stack_10','stack_11', 'stack_12', 'stack_13', 'stack_14', 'stack_15',\n",
    "                     'stack_16','stack_17', 'stack_18', 'stack_19', 'stack_20',\n",
    "                     'name_1', 'name_2', 'name_3', 'name_4', 'name_5',\n",
    "                     'type_1', 'type_2', 'type_3', 'type_4', 'type_5',\n",
    "                     'value_1', 'value_2', 'value_3', 'value_4', 'value_5']].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_num_test = test_data[['processId', 'threadId', 'parentProcessId', 'userId', 'mountNamespace', 'eventId', 'argsNum', 'returnValue', 'stack_address_len']]\n",
    "X_cat_test = test_data[['processName', 'hostName', 'eventName',\n",
    "                     'stack_1', 'stack_2', 'stack_3', 'stack_4',\n",
    "                     'stack_5', 'stack_6', 'stack_7', 'stack_8', 'stack_9', \n",
    "                     'stack_10','stack_11', 'stack_12', 'stack_13', 'stack_14', 'stack_15',\n",
    "                     'stack_16','stack_17', 'stack_18', 'stack_19', 'stack_20',\n",
    "                     'name_1', 'name_2', 'name_3', 'name_4', 'name_5',\n",
    "                     'type_1', 'type_2', 'type_3', 'type_4', 'type_5',\n",
    "                     'value_1', 'value_2', 'value_3', 'value_4', 'value_5']].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Categorical features encoding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stack addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', \n",
    "                                 unknown_value=-1, dtype=float)\n",
    "\n",
    "# Encode Stack adresses\n",
    "stackaddresses_train_enc = stack_ordinal_encoder.fit_transform(X_cat_train[['stack_1', 'stack_2', 'stack_3', 'stack_4', 'stack_5',\n",
    "                                                                      'stack_6', 'stack_7', 'stack_8', 'stack_9', 'stack_10',\n",
    "                                                                      'stack_11', 'stack_12', 'stack_13', 'stack_14', 'stack_15',\n",
    "                                                                      'stack_16','stack_17', 'stack_18', 'stack_19', 'stack_20']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cat_train[['stack_1', 'stack_2', 'stack_3', 'stack_4', 'stack_5',\n",
    "             'stack_6', 'stack_7', 'stack_8', 'stack_9', 'stack_10',\n",
    "             'stack_11', 'stack_12', 'stack_13', 'stack_14', 'stack_15',\n",
    "             'stack_16','stack_17', 'stack_18', 'stack_19', 'stack_20']] = stackaddresses_train_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stackaddresses_val_enc = stack_ordinal_encoder.transform(X_cat_val[['stack_1', 'stack_2', 'stack_3', 'stack_4', 'stack_5',\n",
    "                                                        'stack_6', 'stack_7', 'stack_8', 'stack_9', 'stack_10',\n",
    "                                                        'stack_11', 'stack_12', 'stack_13', 'stack_14', 'stack_15',\n",
    "                                                        'stack_16','stack_17', 'stack_18', 'stack_19', 'stack_20']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stackaddresses_val_enc = np.where(stackaddresses_val_enc==-1, np.max(stackaddresses_train_enc)+1, stackaddresses_val_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cat_val[['stack_1', 'stack_2', 'stack_3', 'stack_4', 'stack_5',\n",
    "           'stack_6', 'stack_7', 'stack_8', 'stack_9', 'stack_10',\n",
    "           'stack_11', 'stack_12', 'stack_13', 'stack_14', 'stack_15',\n",
    "           'stack_16','stack_17', 'stack_18', 'stack_19', 'stack_20']]= stackaddresses_val_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "stackaddresses_test_enc = stack_ordinal_encoder.transform(X_cat_test[['stack_1', 'stack_2', 'stack_3', 'stack_4', 'stack_5',\n",
    "                                                        'stack_6', 'stack_7', 'stack_8', 'stack_9', 'stack_10',\n",
    "                                                        'stack_11', 'stack_12', 'stack_13', 'stack_14', 'stack_15',\n",
    "                                                        'stack_16','stack_17', 'stack_18', 'stack_19', 'stack_20']])\n",
    "\n",
    "stackaddresses_test_enc = np.where(stackaddresses_test_enc==-1, np.max(stackaddresses_train_enc)+1, stackaddresses_test_enc)\n",
    "\n",
    "X_cat_test[['stack_1', 'stack_2', 'stack_3', 'stack_4', 'stack_5',\n",
    "           'stack_6', 'stack_7', 'stack_8', 'stack_9', 'stack_10',\n",
    "           'stack_11', 'stack_12', 'stack_13', 'stack_14', 'stack_15',\n",
    "           'stack_16','stack_17', 'stack_18', 'stack_19', 'stack_20']]= stackaddresses_test_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Stack adresses\n",
    "args_ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1, dtype=float)\n",
    "args_train_enc = args_ordinal_encoder.fit_transform(X_cat_train[['name_1', 'name_2', 'name_3', 'name_4', 'name_5',\n",
    "                                                            'type_1', 'type_2', 'type_3', 'type_4', 'type_5',\n",
    "                                                            'value_1', 'value_2', 'value_3', 'value_4', 'value_5']])\n",
    "X_cat_train[['name_1', 'name_2', 'name_3', 'name_4', 'name_5',\n",
    "             'type_1', 'type_2', 'type_3', 'type_4', 'type_5',\n",
    "             'value_1', 'value_2', 'value_3', 'value_4', 'value_5']] = args_train_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_val_enc = args_ordinal_encoder.transform(X_cat_val[['name_1', 'name_2', 'name_3', 'name_4', 'name_5',\n",
    "                                                    'type_1', 'type_2', 'type_3', 'type_4', 'type_5',\n",
    "                                                    'value_1', 'value_2', 'value_3', 'value_4', 'value_5']])\n",
    "\n",
    "args_val_enc = np.where(args_val_enc==-1, np.max(args_train_enc)+1, args_val_enc)\n",
    "\n",
    "X_cat_val[['name_1', 'name_2', 'name_3', 'name_4', 'name_5',\n",
    "           'type_1', 'type_2', 'type_3', 'type_4', 'type_5',\n",
    "           'value_1', 'value_2', 'value_3', 'value_4', 'value_5']] = args_val_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_test_enc = args_ordinal_encoder.transform(X_cat_test[['name_1', 'name_2', 'name_3', 'name_4', 'name_5',\n",
    "                                                      'type_1', 'type_2', 'type_3', 'type_4', 'type_5',\n",
    "                                                      'value_1', 'value_2', 'value_3', 'value_4', 'value_5']])\n",
    "\n",
    "args_test_enc = np.where(args_test_enc==-1, np.max(args_train_enc)+1, args_test_enc)\n",
    "\n",
    "X_cat_test[['name_1', 'name_2', 'name_3', 'name_4', 'name_5',\n",
    "           'type_1', 'type_2', 'type_3', 'type_4', 'type_5',\n",
    "           'value_1', 'value_2', 'value_3', 'value_4', 'value_5']] = args_test_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ProcessName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1, dtype=float)\n",
    "X_cat_train['processName'] = proc_ordinal_encoder.fit_transform(X_cat_train[['processName']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "processName_val_enc = proc_ordinal_encoder.transform(X_cat_val[['processName']])\n",
    "processName_val_enc = np.where(processName_val_enc==-1, np.max(X_cat_train['processName'])+1, processName_val_enc)\n",
    "X_cat_val['processName'] = processName_val_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "processName_test_enc = proc_ordinal_encoder.transform(X_cat_test[['processName']])\n",
    "processName_test_enc = np.where(processName_test_enc==-1, np.max(X_cat_train['processName'])+1, processName_test_enc)\n",
    "X_cat_test['processName'] = processName_test_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HostName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "host_ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1, dtype=float)\n",
    "X_cat_train['hostName'] = host_ordinal_encoder.fit_transform(X_cat_train[['hostName']])\n",
    "\n",
    "host_val_enc = host_ordinal_encoder.transform(X_cat_val[['hostName']])\n",
    "host_val_enc = np.where(host_val_enc==-1, np.max(X_cat_train['hostName'])+1, host_val_enc)\n",
    "X_cat_val['hostName'] = host_val_enc\n",
    "\n",
    "host_test_enc = host_ordinal_encoder.transform(X_cat_test[['hostName']])\n",
    "host_test_enc = np.where(host_test_enc==-1, np.max(X_cat_train['hostName'])+1, host_test_enc)\n",
    "X_cat_test['hostName'] = host_test_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EventName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1, dtype=float)\n",
    "X_cat_train['eventName'] = event_ordinal_encoder.fit_transform(X_cat_train[['eventName']])\n",
    "\n",
    "event_val_enc = event_ordinal_encoder.transform(X_cat_val[['eventName']])\n",
    "event_val_enc = np.where(event_val_enc==-1, np.max(X_cat_train['eventName'])+1, event_val_enc)\n",
    "X_cat_val['eventName'] = event_val_enc\n",
    "\n",
    "event_test_enc = event_ordinal_encoder.transform(X_cat_test[['eventName']])\n",
    "event_test_enc = np.where(event_test_enc==-1, np.max(X_cat_train['eventName'])+1, event_test_enc)\n",
    "X_cat_test['eventName'] = event_test_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([X_num_train, X_cat_train], axis=1)\n",
    "y_train = train_data['sus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = pd.concat([X_num_val, X_cat_val], axis=1)\n",
    "y_val = val_data['sus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.concat([X_num_test, X_cat_test], axis=1)\n",
    "y_test = test_data['sus']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "188967"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Conv1D.__init__() missing 1 required positional argument: 'kernel_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[110], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m      5\u001b[0m     [\n\u001b[1;32m      6\u001b[0m         layers\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39m(X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],)),\n\u001b[0;32m----> 7\u001b[0m         \u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConv1D\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# kernel_size=7,\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msame\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m            \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     14\u001b[0m         layers\u001b[38;5;241m.\u001b[39mDropout(rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m),\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;66;03m# layers.Conv1D(\u001b[39;00m\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;66;03m#     filters=16,\u001b[39;00m\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;66;03m#     kernel_size=7,\u001b[39;00m\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;66;03m#     padding=\"same\",\u001b[39;00m\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;66;03m#     strides=2,\u001b[39;00m\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;66;03m#     activation=\"relu\",\u001b[39;00m\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;66;03m# ),\u001b[39;00m\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;66;03m# layers.Conv1DTranspose(\u001b[39;00m\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;66;03m#     filters=16,\u001b[39;00m\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;66;03m#     kernel_size=7,\u001b[39;00m\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;66;03m#     padding=\"same\",\u001b[39;00m\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;66;03m#     strides=2,\u001b[39;00m\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;66;03m#     activation=\"relu\",\u001b[39;00m\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;66;03m# ),\u001b[39;00m\n\u001b[1;32m     29\u001b[0m         layers\u001b[38;5;241m.\u001b[39mDropout(rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m),\n\u001b[1;32m     30\u001b[0m         layers\u001b[38;5;241m.\u001b[39mConv1DTranspose(\n\u001b[1;32m     31\u001b[0m             filters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m     32\u001b[0m             kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m,\n\u001b[1;32m     33\u001b[0m             padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     34\u001b[0m             strides\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     35\u001b[0m             activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     36\u001b[0m         ),\n\u001b[1;32m     37\u001b[0m         layers\u001b[38;5;241m.\u001b[39mConv1DTranspose(filters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     38\u001b[0m     ]\n\u001b[1;32m     39\u001b[0m )\n\u001b[1;32m     40\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m), loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n",
      "\u001b[0;31mTypeError\u001b[0m: Conv1D.__init__() missing 1 required positional argument: 'kernel_size'"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Input(shape=(X_train.shape[1],)),\n",
    "        layers.Conv1D(\n",
    "            filters=32,\n",
    "            kernel_size=7,\n",
    "            padding=\"same\",\n",
    "            strides=2,\n",
    "            activation=\"relu\",\n",
    "        ),\n",
    "        layers.Dropout(rate=0.2),\n",
    "        # layers.Conv1D(\n",
    "        #     filters=16,\n",
    "        #     kernel_size=7,\n",
    "        #     padding=\"same\",\n",
    "        #     strides=2,\n",
    "        #     activation=\"relu\",\n",
    "        # ),\n",
    "        # layers.Conv1DTranspose(\n",
    "        #     filters=16,\n",
    "        #     kernel_size=7,\n",
    "        #     padding=\"same\",\n",
    "        #     strides=2,\n",
    "        #     activation=\"relu\",\n",
    "        # ),\n",
    "        layers.Dropout(rate=0.2),\n",
    "        layers.Conv1DTranspose(\n",
    "            filters=32,\n",
    "            kernel_size=7,\n",
    "            padding=\"same\",\n",
    "            strides=2,\n",
    "            activation=\"relu\",\n",
    "        ),\n",
    "        layers.Conv1DTranspose(filters=1, kernel_size=7, padding=\"same\"),\n",
    "    ]\n",
    ")\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=\"mse\")\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"data:0\", shape=(None, 47), dtype=float32). Expected shape (None, 188967, 47), but input has incompatible shape (None, 47)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(None, 47), dtype=float32)\n  • training=True\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5120\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval_loss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Python/Deep learning/Beth cybersecurity/virtual_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:123\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Documents/Python/Deep learning/Beth cybersecurity/virtual_env/lib/python3.10/site-packages/keras/src/models/functional.py:274\u001b[0m, in \u001b[0;36mFunctional._adjust_input_rank\u001b[0;34m(self, flat_inputs)\u001b[0m\n\u001b[1;32m    272\u001b[0m             adjusted\u001b[38;5;241m.\u001b[39mappend(ops\u001b[38;5;241m.\u001b[39mexpand_dims(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    273\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input shape for input \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mref_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but input has incompatible shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    277\u001b[0m     )\n\u001b[1;32m    278\u001b[0m \u001b[38;5;66;03m# Add back metadata.\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(flat_inputs)):\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"data:0\", shape=(None, 47), dtype=float32). Expected shape (None, 188967, 47), but input has incompatible shape (None, 47)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(None, 47), dtype=float32)\n  • training=True\n  • mask=None"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train,\n",
    "    X_train,\n",
    "    epochs=50,\n",
    "    batch_size=5120,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\")\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47,)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=X_train.loc[1]\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 47, 32])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.reshape(x, (-1, 47, 1))\n",
    "y=Dense(32)(y) #24x32\n",
    "#y=Dense(64)(y) #12x16\n",
    "\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 96, 256])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=Dense(94, activation='relu', kernel_initializer='lecun_normal', bias_initializer='random_normal')(x)\n",
    "x=Reshape((94, 1))(x)\n",
    "x=Conv1DTranspose(256, 3, activation='relu', kernel_initializer='lecun_normal', bias_initializer='random_normal')(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ sequential_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)       │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,475,712</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ sequential_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)       │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ sequential_1 (\u001b[38;5;33mSequential\u001b[0m)       │ ?                      │     \u001b[38;5;34m1,475,712\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ sequential_2 (\u001b[38;5;33mSequential\u001b[0m)       │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,475,712</span> (5.63 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,475,712\u001b[0m (5.63 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,475,712</span> (5.63 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,475,712\u001b[0m (5.63 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.layers import Conv1D, Dropout, Flatten, Dense, Reshape, Conv1DTranspose\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Define the encoder part\n",
    "encoder = Sequential([\n",
    "    Conv1D(256, 3, activation='relu', input_shape=(47, 1), kernel_initializer='lecun_normal', bias_initializer='random_normal'),\n",
    "    Dropout(0.2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu', kernel_initializer='lecun_normal', bias_initializer='random_normal'),\n",
    "])\n",
    "\n",
    "# Define the decoder part\n",
    "decoder = Sequential([\n",
    "    Dense(94, activation='relu', kernel_initializer='lecun_normal', bias_initializer='random_normal'),\n",
    "    Reshape((94, 1)),  # Reshape back to the original shape\n",
    "    Conv1DTranspose(256, 3, activation='relu', kernel_initializer='lecun_normal', bias_initializer='random_normal'),\n",
    "    Dropout(0.2),\n",
    "    Conv1DTranspose(1, 3, activation='relu', kernel_initializer='lecun_normal', bias_initializer='random_normal'),  # Output layer\n",
    "])\n",
    "\n",
    "# Combine encoder and decoder to create the autoencoder\n",
    "model2 = Sequential([\n",
    "    encoder,\n",
    "    decoder,\n",
    "])\n",
    "\n",
    "# Compile the autoencoder\n",
    "model2.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Print model summary\n",
    "model2.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mKernel shape must have the same length as input, but received kernel of shape (3, 47, 256) and input of shape (None, 47).\u001b[0m\n\nArguments received by Sequential.call():\n  • args=('<KerasTensor shape=(None, 47), dtype=float32, sparse=None, name=keras_tensor_13>',)\n  • kwargs={'mask': 'None'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5120\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval_loss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Python/Deep learning/Beth cybersecurity/virtual_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:123\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Documents/Python/Deep learning/Beth cybersecurity/virtual_env/lib/python3.10/site-packages/keras/src/ops/operation_utils.py:177\u001b[0m, in \u001b[0;36mcompute_conv_output_shape\u001b[0;34m(input_shape, filters, kernel_size, strides, padding, data_format, dilation_rate)\u001b[0m\n\u001b[1;32m    175\u001b[0m     kernel_shape \u001b[38;5;241m=\u001b[39m kernel_size \u001b[38;5;241m+\u001b[39m (input_shape[\u001b[38;5;241m1\u001b[39m], filters)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kernel_shape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(input_shape):\n\u001b[0;32m--> 177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKernel shape must have the same length as input, but received \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkernel of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkernel_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    181\u001b[0m     )\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dilation_rate, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    183\u001b[0m     dilation_rate \u001b[38;5;241m=\u001b[39m (dilation_rate,) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(spatial_shape)\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mKernel shape must have the same length as input, but received kernel of shape (3, 47, 256) and input of shape (None, 47).\u001b[0m\n\nArguments received by Sequential.call():\n  • args=('<KerasTensor shape=(None, 47), dtype=float32, sparse=None, name=keras_tensor_13>',)\n  • kwargs={'mask': 'None'}"
     ]
    }
   ],
   "source": [
    "history = model2.fit(\n",
    "    X_train,\n",
    "    X_train,\n",
    "    epochs=50,\n",
    "    batch_size=5120,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\")\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(Model):\n",
    "  def __init__(self, latent_dim, shape):\n",
    "    super(Autoencoder, self).__init__()\n",
    "    self.latent_dim = latent_dim\n",
    "    self.shape = shape\n",
    "    self.encoder = tf.keras.Sequential([\n",
    "      layers.Flatten(),\n",
    "      layers.Dense(latent_dim, activation='relu'),\n",
    "    ])\n",
    "    self.decoder = tf.keras.Sequential([\n",
    "      layers.Dense(tf.math.reduce_prod(shape).numpy(), activation='sigmoid'),\n",
    "      layers.Reshape(shape)\n",
    "    ])\n",
    "\n",
    "  def call(self, x):\n",
    "    encoded = self.encoder(x)\n",
    "    decoded = self.decoder(encoded)\n",
    "    return decoded\n",
    "\n",
    "\n",
    "shape = X_train.shape[1:]\n",
    "latent_dim = 32\n",
    "autoencoder = Autoencoder(latent_dim, shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(Model):\n",
    "    def __init__(self, latent_dim, shape):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.shape = shape\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(128, activation='relu'),  # Added dense layer\n",
    "            layers.Dense(latent_dim, activation='relu'),\n",
    "        ])\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.Dense(128, activation='relu'),  # Added dense layer\n",
    "            layers.Dense(tf.math.reduce_prod(shape).numpy(), activation='sigmoid'),\n",
    "            layers.Reshape(shape)\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "shape = X_train.shape[1:]\n",
    "latent_dim = 32\n",
    "autoencoder = Autoencoder(latent_dim, shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, losses\n",
    "autoencoder.compile(optimizer='adam', loss=losses.MeanSquaredError())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m5906/5906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 551us/step - loss: 4175302.7500 - val_loss: 24928528.0000\n",
      "Epoch 2/10\n",
      "\u001b[1m5906/5906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 534us/step - loss: 4184634.5000 - val_loss: 24928528.0000\n",
      "Epoch 3/10\n",
      "\u001b[1m5906/5906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 532us/step - loss: 4184705.0000 - val_loss: 24928528.0000\n",
      "Epoch 4/10\n",
      "\u001b[1m5906/5906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 527us/step - loss: 4179950.5000 - val_loss: 24928528.0000\n",
      "Epoch 5/10\n",
      "\u001b[1m5906/5906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 537us/step - loss: 4176650.2500 - val_loss: 24928528.0000\n",
      "Epoch 6/10\n",
      "\u001b[1m5906/5906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 529us/step - loss: 4173737.2500 - val_loss: 24928528.0000\n",
      "Epoch 7/10\n",
      "\u001b[1m5906/5906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 527us/step - loss: 4187770.5000 - val_loss: 24928528.0000\n",
      "Epoch 8/10\n",
      "\u001b[1m5906/5906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 525us/step - loss: 4178652.5000 - val_loss: 24928528.0000\n",
      "Epoch 9/10\n",
      "\u001b[1m5906/5906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 529us/step - loss: 4168637.5000 - val_loss: 24928528.0000\n",
      "Epoch 10/10\n",
      "\u001b[1m5906/5906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 534us/step - loss: 4185520.2500 - val_loss: 24928528.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x37e43b070>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(X_train, X_train,\n",
    "                epochs=10,\n",
    "                shuffle=True,\n",
    "                validation_data=(X_val, X_val))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
