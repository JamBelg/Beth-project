{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.layers import Flatten, Dense, Input, Dropout, Conv1D, BatchNormalization, MaxPooling1D, Flatten, LSTM, Bidirectional, Embedding, Concatenate, Reshape\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam, SGD, RMSprop, Adamax\n",
    "\n",
    "import ast\n",
    "\n",
    "# Hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../../../Data/labelled_training_data.csv')\n",
    "val_data = pd.read_csv('../../../Data/labelled_validation_data.csv')\n",
    "test_data = pd.read_csv('../../../Data/labelled_testing_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"processId\"] = train_data[\"processId\"].map(lambda x: 0 if x in [0, 1, 2] else 1)  # Map to OS/not OS\n",
    "train_data[\"parentProcessId\"] = train_data[\"parentProcessId\"].map(lambda x: 0 if x in [0, 1, 2] else 1)  # Map to OS/not OS\n",
    "train_data[\"userId\"] = train_data[\"userId\"].map(lambda x: 0 if x < 1000 else 1)  # Map to OS/not OS\n",
    "train_data[\"mountNamespace\"] = train_data[\"mountNamespace\"].map(lambda x: 0 if x == 4026531840 else 1)  # Map to mount access to mnt/ (all non-OS users) /elsewhere\n",
    "train_data[\"eventId\"] = train_data[\"eventId\"]  # Keep eventId values (requires knowing max value)\n",
    "train_data[\"returnValue\"] = train_data[\"returnValue\"].map(lambda x: 0 if x == 0 else (1 if x > 0 else 2))\n",
    "\n",
    "\n",
    "val_data[\"processId\"] = val_data[\"processId\"].map(lambda x: 0 if x in [0, 1, 2] else 1)  # Map to OS/not OS\n",
    "val_data[\"parentProcessId\"] = val_data[\"parentProcessId\"].map(lambda x: 0 if x in [0, 1, 2] else 1)  # Map to OS/not OS\n",
    "val_data[\"userId\"] = val_data[\"userId\"].map(lambda x: 0 if x < 1000 else 1)  # Map to OS/not OS\n",
    "val_data[\"mountNamespace\"] = val_data[\"mountNamespace\"].map(lambda x: 0 if x == 4026531840 else 1)  # Map to mount access to mnt/ (all non-OS users) /elsewhere\n",
    "val_data[\"eventId\"] = val_data[\"eventId\"]  # Keep eventId values (requires knowing max value)\n",
    "val_data[\"returnValue\"] = val_data[\"returnValue\"].map(lambda x: 0 if x == 0 else (1 if x > 0 else 2)) \n",
    "\n",
    "\n",
    "test_data[\"processId\"] = test_data[\"processId\"].map(lambda x: 0 if x in [0, 1, 2] else 1)  # Map to OS/not OS\n",
    "test_data[\"parentProcessId\"] = test_data[\"parentProcessId\"].map(lambda x: 0 if x in [0, 1, 2] else 1)  # Map to OS/not OS\n",
    "test_data[\"userId\"] = test_data[\"userId\"].map(lambda x: 0 if x < 1000 else 1)  # Map to OS/not OS\n",
    "test_data[\"mountNamespace\"] = test_data[\"mountNamespace\"].map(lambda x: 0 if x == 4026531840 else 1)  # Map to mount access to mnt/ (all non-OS users) /elsewhere\n",
    "test_data[\"eventId\"] = test_data[\"eventId\"]  # Keep eventId values (requires knowing max value)\n",
    "test_data[\"returnValue\"] = test_data[\"returnValue\"].map(lambda x: 0 if x == 0 else (1 if x > 0 else 2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>processId</th>\n",
       "      <th>threadId</th>\n",
       "      <th>parentProcessId</th>\n",
       "      <th>userId</th>\n",
       "      <th>mountNamespace</th>\n",
       "      <th>eventId</th>\n",
       "      <th>argsNum</th>\n",
       "      <th>returnValue</th>\n",
       "      <th>sus</th>\n",
       "      <th>evil</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>763144.000000</td>\n",
       "      <td>763144.000000</td>\n",
       "      <td>763144.000000</td>\n",
       "      <td>763144.000000</td>\n",
       "      <td>763144.000000</td>\n",
       "      <td>763144.000000</td>\n",
       "      <td>763144.000000</td>\n",
       "      <td>763144.000000</td>\n",
       "      <td>763144.000000</td>\n",
       "      <td>763144.000000</td>\n",
       "      <td>763144.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1844.582673</td>\n",
       "      <td>0.978617</td>\n",
       "      <td>6820.265241</td>\n",
       "      <td>0.895755</td>\n",
       "      <td>0.001305</td>\n",
       "      <td>0.256371</td>\n",
       "      <td>288.158953</td>\n",
       "      <td>2.672082</td>\n",
       "      <td>0.340016</td>\n",
       "      <td>0.001663</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1136.638249</td>\n",
       "      <td>0.144656</td>\n",
       "      <td>1937.068333</td>\n",
       "      <td>0.305578</td>\n",
       "      <td>0.036103</td>\n",
       "      <td>0.436629</td>\n",
       "      <td>385.117778</td>\n",
       "      <td>1.340906</td>\n",
       "      <td>0.533623</td>\n",
       "      <td>0.040744</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>132.560721</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>903.250802</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7313.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1829.203642</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7365.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2761.380825</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7415.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>257.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3954.587643</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8619.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1010.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           timestamp      processId       threadId  parentProcessId  \\\n",
       "count  763144.000000  763144.000000  763144.000000    763144.000000   \n",
       "mean     1844.582673       0.978617    6820.265241         0.895755   \n",
       "std      1136.638249       0.144656    1937.068333         0.305578   \n",
       "min       132.560721       0.000000       1.000000         0.000000   \n",
       "25%       903.250802       1.000000    7313.000000         1.000000   \n",
       "50%      1829.203642       1.000000    7365.000000         1.000000   \n",
       "75%      2761.380825       1.000000    7415.000000         1.000000   \n",
       "max      3954.587643       1.000000    8619.000000         1.000000   \n",
       "\n",
       "              userId  mountNamespace        eventId        argsNum  \\\n",
       "count  763144.000000   763144.000000  763144.000000  763144.000000   \n",
       "mean        0.001305        0.256371     288.158953       2.672082   \n",
       "std         0.036103        0.436629     385.117778       1.340906   \n",
       "min         0.000000        0.000000       3.000000       0.000000   \n",
       "25%         0.000000        0.000000       3.000000       1.000000   \n",
       "50%         0.000000        0.000000      62.000000       3.000000   \n",
       "75%         0.000000        1.000000     257.000000       4.000000   \n",
       "max         1.000000        1.000000    1010.000000       5.000000   \n",
       "\n",
       "         returnValue            sus      evil  \n",
       "count  763144.000000  763144.000000  763144.0  \n",
       "mean        0.340016       0.001663       0.0  \n",
       "std         0.533623       0.040744       0.0  \n",
       "min         0.000000       0.000000       0.0  \n",
       "25%         0.000000       0.000000       0.0  \n",
       "50%         0.000000       0.000000       0.0  \n",
       "75%         1.000000       0.000000       0.0  \n",
       "max         2.000000       1.000000       0.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp          float64\n",
       "processId            int64\n",
       "threadId             int64\n",
       "parentProcessId      int64\n",
       "userId               int64\n",
       "mountNamespace       int64\n",
       "processName         object\n",
       "hostName            object\n",
       "eventId              int64\n",
       "eventName           object\n",
       "stackAddresses      object\n",
       "argsNum              int64\n",
       "returnValue          int64\n",
       "args                object\n",
       "sus                  int64\n",
       "evil                 int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Stackaddress**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112474"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_addresses_df = pd.concat([train_data['stackAddresses'], val_data['stackAddresses'], test_data['stackAddresses']], axis=0)\n",
    "len(stack_addresses_df.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert String to List**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[140662171848350, 11649800180280676]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert string to list\n",
    "train_data.stackAddresses = train_data.stackAddresses.apply(ast.literal_eval)\n",
    "val_data.stackAddresses = val_data.stackAddresses.apply(ast.literal_eval)\n",
    "test_data.stackAddresses = test_data.stackAddresses.apply(ast.literal_eval)\n",
    "train_data.stackAddresses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset - Max length of stack addresses: 20\n",
      "Validation dataset - Max length of stack addresses: 20\n",
      "Testing dataset - Max length of stack addresses: 20\n"
     ]
    }
   ],
   "source": [
    "train_data['stack_address_len']=train_data.stackAddresses.apply(len)\n",
    "val_data['stack_address_len']=val_data.stackAddresses.apply(len)\n",
    "test_data['stack_address_len']=test_data.stackAddresses.apply(len)\n",
    "print(f\"Training dataset - Max length of stack addresses: {max(train_data['stack_address_len'])}\")\n",
    "print(f\"Validation dataset - Max length of stack addresses: {max(val_data['stack_address_len'])}\")\n",
    "print(f\"Testing dataset - Max length of stack addresses: {max(test_data['stack_address_len'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stack_address_len\n",
       "0     521115\n",
       "2     109594\n",
       "1      65545\n",
       "3      59301\n",
       "4       2445\n",
       "20      1406\n",
       "14      1073\n",
       "15       932\n",
       "6        354\n",
       "8        347\n",
       "17       276\n",
       "10       206\n",
       "11       190\n",
       "9        143\n",
       "5         92\n",
       "16        90\n",
       "12        27\n",
       "7          8\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['stack_address_len'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(max(train_data['stack_address_len'])):\n",
    "    train_data[f\"stack_{i+1}\"]=\"\"\n",
    "    val_data[f\"stack_{i+1}\"]=\"\"\n",
    "    test_data[f\"stack_{i+1}\"]=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in train_data.iterrows():\n",
    "    list_stack = [float(elem) for elem in row['stackAddresses']]\n",
    "    for i, elem in enumerate(list_stack):\n",
    "        train_data.at[index, f'stack_{i+1}'] = elem\n",
    "\n",
    "for index, row in val_data.iterrows():\n",
    "    list_stack = [float(elem) for elem in row['stackAddresses']]\n",
    "    for i, elem in enumerate(list_stack):\n",
    "        val_data.at[index, f'stack_{i+1}'] = elem\n",
    "\n",
    "\n",
    "for index, row in test_data.iterrows():\n",
    "    list_stack = [float(elem) for elem in row['stackAddresses']]\n",
    "    for i, elem in enumerate(list_stack):\n",
    "        test_data.at[index, f'stack_{i+1}'] = elem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp                                                  1809.495787\n",
       "processId                                                            1\n",
       "threadId                                                          7337\n",
       "parentProcessId                                                      0\n",
       "userId                                                               0\n",
       "mountNamespace                                                       1\n",
       "processName                                                      close\n",
       "hostName                                               ip-10-100-1-120\n",
       "eventId                                                            157\n",
       "eventName                                                        prctl\n",
       "stackAddresses                    [140662171848350, 11649800180280676]\n",
       "argsNum                                                              5\n",
       "returnValue                                                          0\n",
       "args                 [{'name': 'option', 'type': 'int', 'value': 'P...\n",
       "sus                                                                  1\n",
       "evil                                                                 0\n",
       "stack_address_len                                                    2\n",
       "stack_1                                              140662171848350.0\n",
       "stack_2                                            11649800180280676.0\n",
       "stack_3                                                               \n",
       "stack_4                                                               \n",
       "stack_5                                                               \n",
       "stack_6                                                               \n",
       "stack_7                                                               \n",
       "stack_8                                                               \n",
       "stack_9                                                               \n",
       "stack_10                                                              \n",
       "stack_11                                                              \n",
       "stack_12                                                              \n",
       "stack_13                                                              \n",
       "stack_14                                                              \n",
       "stack_15                                                              \n",
       "stack_16                                                              \n",
       "stack_17                                                              \n",
       "stack_18                                                              \n",
       "stack_19                                                              \n",
       "stack_20                                                              \n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.loc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Args**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "argsNum\n",
       "4    359113\n",
       "1    230609\n",
       "2    149273\n",
       "3     20062\n",
       "5      2678\n",
       "0      1409\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['argsNum'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "214720"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data['args'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[{'name': 'pathname', 'type': 'const char*', 'value': '/etc/ld.so.cache'}, {'name': 'flags', 'type': 'int', 'value': 'O_RDONLY|O_LARGEFILE'}, {'name': 'dev', 'type': 'dev_t', 'value': 211812353}, {'name': 'inode', 'type': 'unsigned long', 'value': 62841}]\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['args'][4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split args**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split the string column into list of dictionaries and create new columns\n",
    "def split_and_expand(row):\n",
    "    if pd.isna(row):\n",
    "        return pd.Series([None] * 15)\n",
    "\n",
    "    dicts = ast.literal_eval(row)\n",
    "    result = {'name_{}'.format(i+1): None for i in range(5)}\n",
    "    result.update({'type_{}'.format(i+1): None for i in range(5)})\n",
    "    result.update({'value_{}'.format(i+1): None for i in range(5)})\n",
    "\n",
    "    for i, d in enumerate(dicts):\n",
    "        if i >= 5:\n",
    "            break\n",
    "        result['name_{}'.format(i+1)] = d.get('name')\n",
    "        result['type_{}'.format(i+1)] = d.get('type')\n",
    "        result['value_{}'.format(i+1)] = d.get('value')\n",
    "\n",
    "    return pd.Series(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_split = train_data['args'].apply(split_and_expand)\n",
    "train_data = pd.concat([train_data, args_split], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_split = val_data['args'].apply(split_and_expand)\n",
    "val_data = pd.concat([val_data, args_split], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_split = test_data['args'].apply(split_and_expand)\n",
    "test_data = pd.concat([test_data, args_split], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data1 = train_data.sample(frac=0.4, ignore_index=True)\n",
    "\n",
    "X_num_train = train_data[['processId', 'threadId', 'parentProcessId', 'userId', 'mountNamespace', 'eventId', 'argsNum', 'returnValue', 'stack_address_len']]\n",
    "X_cat_train = train_data[['processName', 'hostName', 'eventName',\n",
    "                     'stack_1', 'stack_2', 'stack_3', 'stack_4',\n",
    "                     'stack_5', 'stack_6', 'stack_7', 'stack_8', 'stack_9', \n",
    "                     'stack_10','stack_11', 'stack_12', 'stack_13', 'stack_14', 'stack_15',\n",
    "                     'stack_16','stack_17', 'stack_18', 'stack_19', 'stack_20',\n",
    "                     'name_1', 'name_2', 'name_3', 'name_4', 'name_5',\n",
    "                     'type_1', 'type_2', 'type_3', 'type_4', 'type_5',\n",
    "                     'value_1', 'value_2', 'value_3', 'value_4', 'value_5']].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_num_val = val_data[['processId', 'threadId', 'parentProcessId', 'userId', 'mountNamespace', 'eventId', 'argsNum', 'returnValue', 'stack_address_len']]\n",
    "X_cat_val = val_data[['processName', 'hostName', 'eventName',\n",
    "                     'stack_1', 'stack_2', 'stack_3', 'stack_4',\n",
    "                     'stack_5', 'stack_6', 'stack_7', 'stack_8', 'stack_9', \n",
    "                     'stack_10','stack_11', 'stack_12', 'stack_13', 'stack_14', 'stack_15',\n",
    "                     'stack_16','stack_17', 'stack_18', 'stack_19', 'stack_20',\n",
    "                     'name_1', 'name_2', 'name_3', 'name_4', 'name_5',\n",
    "                     'type_1', 'type_2', 'type_3', 'type_4', 'type_5',\n",
    "                     'value_1', 'value_2', 'value_3', 'value_4', 'value_5']].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_num_test = test_data[['processId', 'threadId', 'parentProcessId', 'userId', 'mountNamespace', 'eventId', 'argsNum', 'returnValue', 'stack_address_len']]\n",
    "X_cat_test = test_data[['processName', 'hostName', 'eventName',\n",
    "                     'stack_1', 'stack_2', 'stack_3', 'stack_4',\n",
    "                     'stack_5', 'stack_6', 'stack_7', 'stack_8', 'stack_9', \n",
    "                     'stack_10','stack_11', 'stack_12', 'stack_13', 'stack_14', 'stack_15',\n",
    "                     'stack_16','stack_17', 'stack_18', 'stack_19', 'stack_20',\n",
    "                     'name_1', 'name_2', 'name_3', 'name_4', 'name_5',\n",
    "                     'type_1', 'type_2', 'type_3', 'type_4', 'type_5',\n",
    "                     'value_1', 'value_2', 'value_3', 'value_4', 'value_5']].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Categorical features encoding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stack addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', \n",
    "                                 unknown_value=-1, dtype=float)\n",
    "\n",
    "# Encode Stack adresses\n",
    "stackaddresses_train_enc = stack_ordinal_encoder.fit_transform(X_cat_train[['stack_1', 'stack_2', 'stack_3', 'stack_4', 'stack_5',\n",
    "                                                                      'stack_6', 'stack_7', 'stack_8', 'stack_9', 'stack_10',\n",
    "                                                                      'stack_11', 'stack_12', 'stack_13', 'stack_14', 'stack_15',\n",
    "                                                                      'stack_16','stack_17', 'stack_18', 'stack_19', 'stack_20']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cat_train[['stack_1', 'stack_2', 'stack_3', 'stack_4', 'stack_5',\n",
    "             'stack_6', 'stack_7', 'stack_8', 'stack_9', 'stack_10',\n",
    "             'stack_11', 'stack_12', 'stack_13', 'stack_14', 'stack_15',\n",
    "             'stack_16','stack_17', 'stack_18', 'stack_19', 'stack_20']] = stackaddresses_train_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stackaddresses_val_enc = stack_ordinal_encoder.transform(X_cat_val[['stack_1', 'stack_2', 'stack_3', 'stack_4', 'stack_5',\n",
    "                                                        'stack_6', 'stack_7', 'stack_8', 'stack_9', 'stack_10',\n",
    "                                                        'stack_11', 'stack_12', 'stack_13', 'stack_14', 'stack_15',\n",
    "                                                        'stack_16','stack_17', 'stack_18', 'stack_19', 'stack_20']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stackaddresses_val_enc = np.where(stackaddresses_val_enc==-1, np.max(stackaddresses_train_enc)+1, stackaddresses_val_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cat_val[['stack_1', 'stack_2', 'stack_3', 'stack_4', 'stack_5',\n",
    "           'stack_6', 'stack_7', 'stack_8', 'stack_9', 'stack_10',\n",
    "           'stack_11', 'stack_12', 'stack_13', 'stack_14', 'stack_15',\n",
    "           'stack_16','stack_17', 'stack_18', 'stack_19', 'stack_20']]= stackaddresses_val_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "stackaddresses_test_enc = stack_ordinal_encoder.transform(X_cat_test[['stack_1', 'stack_2', 'stack_3', 'stack_4', 'stack_5',\n",
    "                                                        'stack_6', 'stack_7', 'stack_8', 'stack_9', 'stack_10',\n",
    "                                                        'stack_11', 'stack_12', 'stack_13', 'stack_14', 'stack_15',\n",
    "                                                        'stack_16','stack_17', 'stack_18', 'stack_19', 'stack_20']])\n",
    "\n",
    "stackaddresses_test_enc = np.where(stackaddresses_test_enc==-1, np.max(stackaddresses_train_enc)+1, stackaddresses_test_enc)\n",
    "\n",
    "X_cat_test[['stack_1', 'stack_2', 'stack_3', 'stack_4', 'stack_5',\n",
    "           'stack_6', 'stack_7', 'stack_8', 'stack_9', 'stack_10',\n",
    "           'stack_11', 'stack_12', 'stack_13', 'stack_14', 'stack_15',\n",
    "           'stack_16','stack_17', 'stack_18', 'stack_19', 'stack_20']]= stackaddresses_test_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Stack adresses\n",
    "args_ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1, dtype=float)\n",
    "args_train_enc = args_ordinal_encoder.fit_transform(X_cat_train[['name_1', 'name_2', 'name_3', 'name_4', 'name_5',\n",
    "                                                            'type_1', 'type_2', 'type_3', 'type_4', 'type_5',\n",
    "                                                            'value_1', 'value_2', 'value_3', 'value_4', 'value_5']])\n",
    "X_cat_train[['name_1', 'name_2', 'name_3', 'name_4', 'name_5',\n",
    "             'type_1', 'type_2', 'type_3', 'type_4', 'type_5',\n",
    "             'value_1', 'value_2', 'value_3', 'value_4', 'value_5']] = args_train_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_val_enc = args_ordinal_encoder.transform(X_cat_val[['name_1', 'name_2', 'name_3', 'name_4', 'name_5',\n",
    "                                                    'type_1', 'type_2', 'type_3', 'type_4', 'type_5',\n",
    "                                                    'value_1', 'value_2', 'value_3', 'value_4', 'value_5']])\n",
    "\n",
    "args_val_enc = np.where(args_val_enc==-1, np.max(args_train_enc)+1, args_val_enc)\n",
    "\n",
    "X_cat_val[['name_1', 'name_2', 'name_3', 'name_4', 'name_5',\n",
    "           'type_1', 'type_2', 'type_3', 'type_4', 'type_5',\n",
    "           'value_1', 'value_2', 'value_3', 'value_4', 'value_5']] = args_val_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_test_enc = args_ordinal_encoder.transform(X_cat_test[['name_1', 'name_2', 'name_3', 'name_4', 'name_5',\n",
    "                                                      'type_1', 'type_2', 'type_3', 'type_4', 'type_5',\n",
    "                                                      'value_1', 'value_2', 'value_3', 'value_4', 'value_5']])\n",
    "\n",
    "args_test_enc = np.where(args_test_enc==-1, np.max(args_train_enc)+1, args_test_enc)\n",
    "\n",
    "X_cat_test[['name_1', 'name_2', 'name_3', 'name_4', 'name_5',\n",
    "           'type_1', 'type_2', 'type_3', 'type_4', 'type_5',\n",
    "           'value_1', 'value_2', 'value_3', 'value_4', 'value_5']] = args_test_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ProcessName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1, dtype=float)\n",
    "X_cat_train['processName'] = proc_ordinal_encoder.fit_transform(X_cat_train[['processName']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "processName_val_enc = proc_ordinal_encoder.transform(X_cat_val[['processName']])\n",
    "processName_val_enc = np.where(processName_val_enc==-1, np.max(X_cat_train['processName'])+1, processName_val_enc)\n",
    "X_cat_val['processName'] = processName_val_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "processName_test_enc = proc_ordinal_encoder.transform(X_cat_test[['processName']])\n",
    "processName_test_enc = np.where(processName_test_enc==-1, np.max(X_cat_train['processName'])+1, processName_test_enc)\n",
    "X_cat_test['processName'] = processName_test_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HostName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "host_ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1, dtype=float)\n",
    "X_cat_train['hostName'] = host_ordinal_encoder.fit_transform(X_cat_train[['hostName']])\n",
    "\n",
    "host_val_enc = host_ordinal_encoder.transform(X_cat_val[['hostName']])\n",
    "host_val_enc = np.where(host_val_enc==-1, np.max(X_cat_train['hostName'])+1, host_val_enc)\n",
    "X_cat_val['hostName'] = host_val_enc\n",
    "\n",
    "host_test_enc = host_ordinal_encoder.transform(X_cat_test[['hostName']])\n",
    "host_test_enc = np.where(host_test_enc==-1, np.max(X_cat_train['hostName'])+1, host_test_enc)\n",
    "X_cat_test['hostName'] = host_test_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EventName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1, dtype=float)\n",
    "X_cat_train['eventName'] = event_ordinal_encoder.fit_transform(X_cat_train[['eventName']])\n",
    "\n",
    "event_val_enc = event_ordinal_encoder.transform(X_cat_val[['eventName']])\n",
    "event_val_enc = np.where(event_val_enc==-1, np.max(X_cat_train['eventName'])+1, event_val_enc)\n",
    "X_cat_val['eventName'] = event_val_enc\n",
    "\n",
    "event_test_enc = event_ordinal_encoder.transform(X_cat_test[['eventName']])\n",
    "event_test_enc = np.where(event_test_enc==-1, np.max(X_cat_train['eventName'])+1, event_test_enc)\n",
    "X_cat_test['eventName'] = event_test_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([X_num_train, X_cat_train], axis=1)\n",
    "y_train = train_data['sus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = pd.concat([X_num_val, X_cat_val], axis=1)\n",
    "y_val = val_data['sus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.concat([X_num_test, X_cat_test], axis=1)\n",
    "y_test = test_data['sus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler1 = StandardScaler()\n",
    "X_train_scaled = scaler1.fit_transform(X_train)\n",
    "X_val_scaled = scaler1.transform(X_val)\n",
    "X_test_scaled = scaler1.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8449, 26665, 37, 33, 9)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_unique_stack = (np.max(stackaddresses_train_enc)+2).astype('int')\n",
    "n_unique_args = (np.max(args_train_enc)+2).astype('int')\n",
    "n_unique_proc = (np.max(X_cat_train['processName'])+2).astype('int')\n",
    "n_unique_event = (np.max(X_cat_train['eventName'])+2).astype('int')\n",
    "n_unique_host = (np.max(X_cat_train['hostName'])+2).astype('int')\n",
    "\n",
    "n_unique_stack,n_unique_args,n_unique_proc,n_unique_event,n_unique_host"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        assert self.head_dim * num_heads == embed_dim, \"Embedding dimension must be divisible by number of heads\"\n",
    "        \n",
    "        self.query_fc = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key_fc = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value_fc = nn.Linear(embed_dim, embed_dim)\n",
    "        self.fc_out = nn.Linear(embed_dim, embed_dim)\n",
    "    \n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.head_dim)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        query = self.query_fc(query)\n",
    "        key = self.key_fc(key)\n",
    "        value = self.value_fc(value)\n",
    "        \n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "        \n",
    "        scores = torch.matmul(query, key.permute(0, 1, 3, 2)) / np.sqrt(self.head_dim)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "        \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attention_weights, value)\n",
    "        \n",
    "        context = context.permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.num_heads * self.head_dim)\n",
    "        \n",
    "        return self.fc_out(context)\n",
    "\n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, max_len, embed_dim):\n",
    "        super(PositionalEncoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        \n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-np.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, input_1_vocab_size, input_2_vocab_size, \n",
    "                 input_3_vocab_size, input_4_vocab_size, \n",
    "                 input_5_vocab_size, input_3_input_size, \n",
    "                 embed_dim, num_heads):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        \n",
    "        self.embedding_1 = nn.Embedding(input_1_vocab_size, embed_dim)\n",
    "        self.embedding_2 = nn.Embedding(input_2_vocab_size, embed_dim)\n",
    "        self.embedding_3 = nn.Embedding(input_3_vocab_size, embed_dim)\n",
    "        self.embedding_4 = nn.Embedding(input_4_vocab_size, embed_dim)\n",
    "        self.embedding_5 = nn.Embedding(input_5_vocab_size, embed_dim)\n",
    "        \n",
    "        self.positional_encoder = PositionalEncoder(max_len=max(input_1_vocab_size, input_2_vocab_size,\n",
    "                                                                input_3_vocab_size, input_4_vocab_size,\n",
    "                                                                input_5_vocab_size), embed_dim=embed_dim)\n",
    "        \n",
    "        self.multihead_attention = MultiHeadAttention(embed_dim, num_heads)\n",
    "        \n",
    "        self.fc = nn.Linear(embed_dim, 2)  # Output classes are 2 for binary classification\n",
    "        \n",
    "    def forward(self, input_1, input_2, input_3, input_3, input_3):\n",
    "        embedded_input_1 = self.embedding_1(input_1)\n",
    "        embedded_input_2 = self.embedding_2(input_2)\n",
    "        embedded_input_3 = self.embedding_3(input_3)\n",
    "        embedded_input_4 = self.embedding_4(input_4)\n",
    "        embedded_input_5 = self.embedding_5(input_5)\n",
    "        \n",
    "        embedded_inputs = embedded_input_1 + embedded_input_2\n",
    "        \n",
    "        embedded_inputs = self.positional_encoder(embedded_inputs)\n",
    "        \n",
    "        attention_output = self.multihead_attention(embedded_inputs, embedded_inputs, embedded_inputs)\n",
    "        \n",
    "        output = self.fc(attention_output.mean(dim=1))  # Pooling operation\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Example usage:\n",
    "input_1_vocab_size = n_unique_stack\n",
    "input_2_vocab_size = n_unique_args\n",
    "input_3_vocab_size = n_unique_proc\n",
    "input_4_vocab_size = n_unique_event\n",
    "input_5_vocab_size = n_unique_host\n",
    "input_3_input_size = 9\n",
    "embed_dim = 32\n",
    "num_heads = 4\n",
    "\n",
    "model = TransformerClassifier(input_1_vocab_size, input_2_vocab_size, input_3_input_size, embed_dim, num_heads)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "class MyTransformer(nn.Module):\n",
    "  def __init__(self, vocab_size_stack, vocab_size_args, vocab_size_proc, vocab_size_event, vocab_size_host, \n",
    "               embedding_dim, num_heads, ff_dim, num_encoder_blocks, num_features):\n",
    "    super().__init__()\n",
    "\n",
    "    # Embedding layers for categorical features\n",
    "    self.embedding_stack = nn.Embedding(vocab_size_stack, embedding_dim)\n",
    "    self.embedding_args = nn.Embedding(vocab_size_args, embedding_dim)\n",
    "    self.embedding_proc = nn.Embedding(vocab_size_proc, embedding_dim)\n",
    "    self.embedding_event = nn.Embedding(vocab_size_event, embedding_dim)\n",
    "    self.embedding_host = nn.Embedding(vocab_size_host, embedding_dim)\n",
    "\n",
    "    # Linear layer for numerical feature\n",
    "    self.linear_num = nn.Linear(num_features, embedding_dim)\n",
    "\n",
    "    # Positional encoding (implement this using manual or library approach)\n",
    "    # ... (refer to previous explanation for implementation options)\n",
    "\n",
    "    # Transformer encoder block\n",
    "    self.transformer_encoder = nn.TransformerEncoder(TransformerEncoderLayer(embedding_dim, num_heads, ff_dim), \n",
    "                                                     num_layers=num_encoder_blocks)\n",
    "\n",
    "    # Output layer\n",
    "    self.output_layer = nn.Linear(embedding_dim, 1)  # Assuming binary classification\n",
    "\n",
    "  def forward(self, input_stack, input_args, input_proc, input_event, input_host, input_num):\n",
    "    # Embed categorical features\n",
    "    embedded_stack = self.embedding_stack(input_stack)\n",
    "    embedded_args = self.embedding_args(input_args)\n",
    "    embedded_proc = self.embedding_proc(input_proc)\n",
    "    embedded_event = self.embedding_event(input_event)\n",
    "    embedded_host = self.embedding_host(input_host)\n",
    "\n",
    "    # Process numerical feature\n",
    "    embedded_num = self.linear_num(input_num)\n",
    "\n",
    "    # Concatenate embeddings\n",
    "    concatenated = torch.cat([embedded_stack, embedded_args, embedded_proc, embedded_event, embedded_host], dim=1)\n",
    "\n",
    "    # Apply positional encoding (replace ... with your implementation)\n",
    "    encoded_embeddings = self.positional_encoding(concatenated)  # ...\n",
    "\n",
    "    # Pass through transformer encoder\n",
    "    encoded_embeddings = self.transformer_encoder(encoded_embeddings)\n",
    "\n",
    "    # Output layer\n",
    "    outputs = self.output_layer(encoded_embeddings)\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyTransformer(\n",
       "  (embedding_stack): Embedding(8449, 128)\n",
       "  (embedding_args): Embedding(26665, 128)\n",
       "  (embedding_proc): Embedding(37, 128)\n",
       "  (embedding_event): Embedding(33, 128)\n",
       "  (embedding_host): Embedding(9, 128)\n",
       "  (linear_num): Linear(in_features=9, out_features=128, bias=True)\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=32, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=32, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output_layer): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage (assuming your data is in PyTorch tensors)\n",
    "model = MyTransformer(n_unique_stack,\n",
    "                      n_unique_args,\n",
    "                      n_unique_proc,\n",
    "                      n_unique_event,\n",
    "                      n_unique_host, embedding_dim=128,\n",
    "                      num_heads=4,\n",
    "                      ff_dim=32,\n",
    "                      num_encoder_blocks=1,\n",
    "                      num_features=9)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(input_stack_tensor, input_args_tensor, ..., input_num_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TransformerEncoder' from 'transformers' (/Users/jamelbelgacem/Documents/Python/Deep learning/Beth cybersecurity/virtual_env/lib/python3.10/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TransformerEncoder, TransformerEncoderLayer\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mMyTransformer\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'TransformerEncoder' from 'transformers' (/Users/jamelbelgacem/Documents/Python/Deep learning/Beth cybersecurity/virtual_env/lib/python3.10/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import TransformerEncoder, TransformerEncoderLayer\n",
    "from torchvision import transforms\n",
    "\n",
    "class MyTransformer(nn.Module):\n",
    "  def __init__(self, vocab_size_stack, vocab_size_args, vocab_size_proc, vocab_size_event, vocab_size_host, \n",
    "               embedding_dim, num_heads, ff_dim, num_encoder_blocks, num_features):\n",
    "    super().__init__()\n",
    "\n",
    "    # Embedding layers for categorical features\n",
    "    self.embedding_stack = nn.Embedding(vocab_size_stack, embedding_dim)\n",
    "    self.embedding_args = nn.Embedding(vocab_size_args, embedding_dim)\n",
    "    self.embedding_proc = nn.Embedding(vocab_size_proc, embedding_dim)\n",
    "    self.embedding_event = nn.Embedding(vocab_size_event, embedding_dim)\n",
    "    self.embedding_host = nn.Embedding(vocab_size_host, embedding_dim)\n",
    "\n",
    "    # Linear layer for numerical feature\n",
    "    self.linear_num = nn.Linear(num_features, embedding_dim)\n",
    "\n",
    "    # Positional encoding layer using torchvision.transforms\n",
    "    self.positional_encoding = transforms.PositionalEncoding(d_model=embedding_dim, max_len=...)  # Replace ... with your maximum sequence length\n",
    "\n",
    "    # Transformer encoder block\n",
    "    self.transformer_encoder = nn.TransformerEncoder(TransformerEncoderLayer(embedding_dim, num_heads, ff_dim), num_layers=num_encoder_blocks)\n",
    "\n",
    "    # Output layer\n",
    "    self.output_layer = nn.Linear(embedding_dim, 1)  # Assuming binary classification\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
