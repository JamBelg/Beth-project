{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.layers import Flatten, Dense, Input, Dropout, Conv1D, BatchNormalization, MaxPooling1D, Flatten, LSTM, Bidirectional, Embedding, Concatenate, Reshape\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam, SGD, RMSprop, Adamax\n",
    "\n",
    "import ast\n",
    "\n",
    "# Hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../../../Data/labelled_training_data.csv')\n",
    "val_data = pd.read_csv('../../../Data/labelled_validation_data.csv')\n",
    "test_data = pd.read_csv('../../../Data/labelled_testing_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"processId\"] = train_data[\"processId\"].map(lambda x: 0 if x in [0, 1, 2] else 1)  # Map to OS/not OS\n",
    "train_data[\"parentProcessId\"] = train_data[\"parentProcessId\"].map(lambda x: 0 if x in [0, 1, 2] else 1)  # Map to OS/not OS\n",
    "train_data[\"userId\"] = train_data[\"userId\"].map(lambda x: 0 if x < 1000 else 1)  # Map to OS/not OS\n",
    "train_data[\"mountNamespace\"] = train_data[\"mountNamespace\"].map(lambda x: 0 if x == 4026531840 else 1)  # Map to mount access to mnt/ (all non-OS users) /elsewhere\n",
    "train_data[\"eventId\"] = train_data[\"eventId\"]  # Keep eventId values (requires knowing max value)\n",
    "train_data[\"returnValue\"] = train_data[\"returnValue\"].map(lambda x: 0 if x == 0 else (1 if x > 0 else 2))\n",
    "\n",
    "\n",
    "val_data[\"processId\"] = val_data[\"processId\"].map(lambda x: 0 if x in [0, 1, 2] else 1)  # Map to OS/not OS\n",
    "val_data[\"parentProcessId\"] = val_data[\"parentProcessId\"].map(lambda x: 0 if x in [0, 1, 2] else 1)  # Map to OS/not OS\n",
    "val_data[\"userId\"] = val_data[\"userId\"].map(lambda x: 0 if x < 1000 else 1)  # Map to OS/not OS\n",
    "val_data[\"mountNamespace\"] = val_data[\"mountNamespace\"].map(lambda x: 0 if x == 4026531840 else 1)  # Map to mount access to mnt/ (all non-OS users) /elsewhere\n",
    "val_data[\"eventId\"] = val_data[\"eventId\"]  # Keep eventId values (requires knowing max value)\n",
    "val_data[\"returnValue\"] = val_data[\"returnValue\"].map(lambda x: 0 if x == 0 else (1 if x > 0 else 2)) \n",
    "\n",
    "\n",
    "test_data[\"processId\"] = test_data[\"processId\"].map(lambda x: 0 if x in [0, 1, 2] else 1)  # Map to OS/not OS\n",
    "test_data[\"parentProcessId\"] = test_data[\"parentProcessId\"].map(lambda x: 0 if x in [0, 1, 2] else 1)  # Map to OS/not OS\n",
    "test_data[\"userId\"] = test_data[\"userId\"].map(lambda x: 0 if x < 1000 else 1)  # Map to OS/not OS\n",
    "test_data[\"mountNamespace\"] = test_data[\"mountNamespace\"].map(lambda x: 0 if x == 4026531840 else 1)  # Map to mount access to mnt/ (all non-OS users) /elsewhere\n",
    "test_data[\"eventId\"] = test_data[\"eventId\"]  # Keep eventId values (requires knowing max value)\n",
    "test_data[\"returnValue\"] = test_data[\"returnValue\"].map(lambda x: 0 if x == 0 else (1 if x > 0 else 2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>processId</th>\n",
       "      <th>threadId</th>\n",
       "      <th>parentProcessId</th>\n",
       "      <th>userId</th>\n",
       "      <th>mountNamespace</th>\n",
       "      <th>eventId</th>\n",
       "      <th>argsNum</th>\n",
       "      <th>returnValue</th>\n",
       "      <th>sus</th>\n",
       "      <th>evil</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>188967.000000</td>\n",
       "      <td>188967.000000</td>\n",
       "      <td>188967.000000</td>\n",
       "      <td>188967.000000</td>\n",
       "      <td>188967.000000</td>\n",
       "      <td>188967.000000</td>\n",
       "      <td>188967.000000</td>\n",
       "      <td>188967.000000</td>\n",
       "      <td>188967.000000</td>\n",
       "      <td>188967.000000</td>\n",
       "      <td>188967.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>455.070015</td>\n",
       "      <td>0.989046</td>\n",
       "      <td>7347.754089</td>\n",
       "      <td>0.950499</td>\n",
       "      <td>0.853329</td>\n",
       "      <td>0.001752</td>\n",
       "      <td>88.802198</td>\n",
       "      <td>2.894569</td>\n",
       "      <td>1.620675</td>\n",
       "      <td>0.907349</td>\n",
       "      <td>0.838411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>61.824198</td>\n",
       "      <td>0.104088</td>\n",
       "      <td>1108.656349</td>\n",
       "      <td>0.216912</td>\n",
       "      <td>0.353779</td>\n",
       "      <td>0.041816</td>\n",
       "      <td>199.137059</td>\n",
       "      <td>0.638079</td>\n",
       "      <td>0.745558</td>\n",
       "      <td>0.289944</td>\n",
       "      <td>0.368074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>129.050634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>461.577225</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7555.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>470.889349</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7555.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>487.116843</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7555.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>496.629091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7705.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1010.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           timestamp      processId       threadId  parentProcessId  \\\n",
       "count  188967.000000  188967.000000  188967.000000    188967.000000   \n",
       "mean      455.070015       0.989046    7347.754089         0.950499   \n",
       "std        61.824198       0.104088    1108.656349         0.216912   \n",
       "min       129.050634       0.000000       1.000000         0.000000   \n",
       "25%       461.577225       1.000000    7555.000000         1.000000   \n",
       "50%       470.889349       1.000000    7555.000000         1.000000   \n",
       "75%       487.116843       1.000000    7555.000000         1.000000   \n",
       "max       496.629091       1.000000    7705.000000         1.000000   \n",
       "\n",
       "              userId  mountNamespace        eventId        argsNum  \\\n",
       "count  188967.000000   188967.000000  188967.000000  188967.000000   \n",
       "mean        0.853329        0.001752      88.802198       2.894569   \n",
       "std         0.353779        0.041816     199.137059       0.638079   \n",
       "min         0.000000        0.000000       2.000000       0.000000   \n",
       "25%         1.000000        0.000000      42.000000       3.000000   \n",
       "50%         1.000000        0.000000      42.000000       3.000000   \n",
       "75%         1.000000        0.000000      42.000000       3.000000   \n",
       "max         1.000000        1.000000    1010.000000       5.000000   \n",
       "\n",
       "         returnValue            sus           evil  \n",
       "count  188967.000000  188967.000000  188967.000000  \n",
       "mean        1.620675       0.907349       0.838411  \n",
       "std         0.745558       0.289944       0.368074  \n",
       "min         0.000000       0.000000       0.000000  \n",
       "25%         2.000000       1.000000       1.000000  \n",
       "50%         2.000000       1.000000       1.000000  \n",
       "75%         2.000000       1.000000       1.000000  \n",
       "max         2.000000       1.000000       1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp          float64\n",
       "processId            int64\n",
       "threadId             int64\n",
       "parentProcessId      int64\n",
       "userId               int64\n",
       "mountNamespace       int64\n",
       "processName         object\n",
       "hostName            object\n",
       "eventId              int64\n",
       "eventName           object\n",
       "stackAddresses      object\n",
       "argsNum              int64\n",
       "returnValue          int64\n",
       "args                object\n",
       "sus                  int64\n",
       "evil                 int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Stackaddress**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15426"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_addresses_df = pd.concat([train_data['stackAddresses'], val_data['stackAddresses'], test_data['stackAddresses']], axis=0)\n",
    "len(stack_addresses_df.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert String to List**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[140159195621643, 140159192455417, 94656731598592]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert string to list\n",
    "train_data.stackAddresses = train_data.stackAddresses.apply(ast.literal_eval)\n",
    "val_data.stackAddresses = val_data.stackAddresses.apply(ast.literal_eval)\n",
    "test_data.stackAddresses = test_data.stackAddresses.apply(ast.literal_eval)\n",
    "train_data.stackAddresses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset - Max length of stack addresses: 20\n",
      "Validation dataset - Max length of stack addresses: 20\n",
      "Testing dataset - Max length of stack addresses: 20\n"
     ]
    }
   ],
   "source": [
    "train_data['stack_address_len']=train_data.stackAddresses.apply(len)\n",
    "val_data['stack_address_len']=val_data.stackAddresses.apply(len)\n",
    "test_data['stack_address_len']=test_data.stackAddresses.apply(len)\n",
    "print(f\"Training dataset - Max length of stack addresses: {max(train_data['stack_address_len'])}\")\n",
    "print(f\"Validation dataset - Max length of stack addresses: {max(val_data['stack_address_len'])}\")\n",
    "print(f\"Testing dataset - Max length of stack addresses: {max(test_data['stack_address_len'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stack_address_len\n",
       "0     181438\n",
       "2       3428\n",
       "1       1991\n",
       "3       1748\n",
       "4        154\n",
       "20        50\n",
       "6         36\n",
       "15        28\n",
       "14        23\n",
       "5         19\n",
       "7         14\n",
       "8         13\n",
       "17         6\n",
       "11         6\n",
       "10         4\n",
       "16         4\n",
       "12         3\n",
       "9          2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['stack_address_len'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(max(train_data['stack_address_len'])):\n",
    "    train_data[f\"stack_{i+1}\"]=\"\"\n",
    "    val_data[f\"stack_{i+1}\"]=\"\"\n",
    "    test_data[f\"stack_{i+1}\"]=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in train_data.iterrows():\n",
    "    list_stack = [float(elem) for elem in row['stackAddresses']]\n",
    "    for i, elem in enumerate(list_stack):\n",
    "        train_data.at[index, f'stack_{i+1}'] = elem\n",
    "\n",
    "for index, row in val_data.iterrows():\n",
    "    list_stack = [float(elem) for elem in row['stackAddresses']]\n",
    "    for i, elem in enumerate(list_stack):\n",
    "        val_data.at[index, f'stack_{i+1}'] = elem\n",
    "\n",
    "\n",
    "for index, row in test_data.iterrows():\n",
    "    list_stack = [float(elem) for elem in row['stackAddresses']]\n",
    "    for i, elem in enumerate(list_stack):\n",
    "        test_data.at[index, f'stack_{i+1}'] = elem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp                                                   129.050634\n",
       "processId                                                            1\n",
       "threadId                                                           382\n",
       "parentProcessId                                                      0\n",
       "userId                                                               0\n",
       "mountNamespace                                                       1\n",
       "processName                                            systemd-resolve\n",
       "hostName                                               ip-10-100-1-217\n",
       "eventId                                                             41\n",
       "eventName                                                       socket\n",
       "stackAddresses       [140159195621643, 140159192455417, 94656731598...\n",
       "argsNum                                                              3\n",
       "returnValue                                                          1\n",
       "args                 [{'name': 'domain', 'type': 'int', 'value': 'A...\n",
       "sus                                                                  0\n",
       "evil                                                                 0\n",
       "stack_address_len                                                    3\n",
       "stack_1                                              140159195621643.0\n",
       "stack_2                                              140159192455417.0\n",
       "stack_3                                               94656731598592.0\n",
       "stack_4                                                               \n",
       "stack_5                                                               \n",
       "stack_6                                                               \n",
       "stack_7                                                               \n",
       "stack_8                                                               \n",
       "stack_9                                                               \n",
       "stack_10                                                              \n",
       "stack_11                                                              \n",
       "stack_12                                                              \n",
       "stack_13                                                              \n",
       "stack_14                                                              \n",
       "stack_15                                                              \n",
       "stack_16                                                              \n",
       "stack_17                                                              \n",
       "stack_18                                                              \n",
       "stack_19                                                              \n",
       "stack_20                                                              \n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.loc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Args**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "argsNum\n",
       "3    148719\n",
       "4     15147\n",
       "2     12493\n",
       "1     11707\n",
       "5       708\n",
       "0       193\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['argsNum'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35177"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data['args'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[{'name': 'fd', 'type': 'int', 'value': 17}, {'name': 'statbuf', 'type': 'struct stat*', 'value': '0x7FFE8293A360'}]\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['args'][4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split args**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split the string column into list of dictionaries and create new columns\n",
    "def split_and_expand(row):\n",
    "    if pd.isna(row):\n",
    "        return pd.Series([None] * 15)\n",
    "\n",
    "    dicts = ast.literal_eval(row)\n",
    "    result = {'name_{}'.format(i+1): None for i in range(5)}\n",
    "    result.update({'type_{}'.format(i+1): None for i in range(5)})\n",
    "    result.update({'value_{}'.format(i+1): None for i in range(5)})\n",
    "\n",
    "    for i, d in enumerate(dicts):\n",
    "        if i >= 5:\n",
    "            break\n",
    "        result['name_{}'.format(i+1)] = d.get('name')\n",
    "        result['type_{}'.format(i+1)] = d.get('type')\n",
    "        result['value_{}'.format(i+1)] = d.get('value')\n",
    "\n",
    "    return pd.Series(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_split = train_data['args'].apply(split_and_expand)\n",
    "train_data = pd.concat([train_data, args_split], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_split = val_data['args'].apply(split_and_expand)\n",
    "val_data = pd.concat([val_data, args_split], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_split = test_data['args'].apply(split_and_expand)\n",
    "test_data = pd.concat([test_data, args_split], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data1 = train_data.sample(frac=0.4, ignore_index=True)\n",
    "\n",
    "X_num_train = train_data[['processId', 'threadId', 'parentProcessId', 'userId', 'mountNamespace', 'eventId', 'argsNum', 'returnValue', 'stack_address_len']]\n",
    "X_cat_train = train_data[['processName', 'hostName', 'eventName',\n",
    "                     'stack_1', 'stack_2', 'stack_3', 'stack_4',\n",
    "                     'stack_5', 'stack_6', 'stack_7', 'stack_8', 'stack_9', \n",
    "                     'stack_10','stack_11', 'stack_12', 'stack_13', 'stack_14', 'stack_15',\n",
    "                     'stack_16','stack_17', 'stack_18', 'stack_19', 'stack_20',\n",
    "                     'name_1', 'name_2', 'name_3', 'name_4', 'name_5',\n",
    "                     'type_1', 'type_2', 'type_3', 'type_4', 'type_5',\n",
    "                     'value_1', 'value_2', 'value_3', 'value_4', 'value_5']].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_num_val = val_data[['processId', 'threadId', 'parentProcessId', 'userId', 'mountNamespace', 'eventId', 'argsNum', 'returnValue', 'stack_address_len']]\n",
    "X_cat_val = val_data[['processName', 'hostName', 'eventName',\n",
    "                     'stack_1', 'stack_2', 'stack_3', 'stack_4',\n",
    "                     'stack_5', 'stack_6', 'stack_7', 'stack_8', 'stack_9', \n",
    "                     'stack_10','stack_11', 'stack_12', 'stack_13', 'stack_14', 'stack_15',\n",
    "                     'stack_16','stack_17', 'stack_18', 'stack_19', 'stack_20',\n",
    "                     'name_1', 'name_2', 'name_3', 'name_4', 'name_5',\n",
    "                     'type_1', 'type_2', 'type_3', 'type_4', 'type_5',\n",
    "                     'value_1', 'value_2', 'value_3', 'value_4', 'value_5']].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_num_test = test_data[['processId', 'threadId', 'parentProcessId', 'userId', 'mountNamespace', 'eventId', 'argsNum', 'returnValue', 'stack_address_len']]\n",
    "X_cat_test = test_data[['processName', 'hostName', 'eventName',\n",
    "                     'stack_1', 'stack_2', 'stack_3', 'stack_4',\n",
    "                     'stack_5', 'stack_6', 'stack_7', 'stack_8', 'stack_9', \n",
    "                     'stack_10','stack_11', 'stack_12', 'stack_13', 'stack_14', 'stack_15',\n",
    "                     'stack_16','stack_17', 'stack_18', 'stack_19', 'stack_20',\n",
    "                     'name_1', 'name_2', 'name_3', 'name_4', 'name_5',\n",
    "                     'type_1', 'type_2', 'type_3', 'type_4', 'type_5',\n",
    "                     'value_1', 'value_2', 'value_3', 'value_4', 'value_5']].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Categorical features encoding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stack addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', \n",
    "                                 unknown_value=-1, dtype=float)\n",
    "\n",
    "# Encode Stack adresses\n",
    "stackaddresses_train_enc = stack_ordinal_encoder.fit_transform(X_cat_train[['stack_1', 'stack_2', 'stack_3', 'stack_4', 'stack_5',\n",
    "                                                                      'stack_6', 'stack_7', 'stack_8', 'stack_9', 'stack_10',\n",
    "                                                                      'stack_11', 'stack_12', 'stack_13', 'stack_14', 'stack_15',\n",
    "                                                                      'stack_16','stack_17', 'stack_18', 'stack_19', 'stack_20']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cat_train[['stack_1', 'stack_2', 'stack_3', 'stack_4', 'stack_5',\n",
    "             'stack_6', 'stack_7', 'stack_8', 'stack_9', 'stack_10',\n",
    "             'stack_11', 'stack_12', 'stack_13', 'stack_14', 'stack_15',\n",
    "             'stack_16','stack_17', 'stack_18', 'stack_19', 'stack_20']] = stackaddresses_train_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stackaddresses_val_enc = stack_ordinal_encoder.transform(X_cat_val[['stack_1', 'stack_2', 'stack_3', 'stack_4', 'stack_5',\n",
    "                                                        'stack_6', 'stack_7', 'stack_8', 'stack_9', 'stack_10',\n",
    "                                                        'stack_11', 'stack_12', 'stack_13', 'stack_14', 'stack_15',\n",
    "                                                        'stack_16','stack_17', 'stack_18', 'stack_19', 'stack_20']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stackaddresses_val_enc = np.where(stackaddresses_val_enc==-1, np.max(stackaddresses_train_enc)+1, stackaddresses_val_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cat_val[['stack_1', 'stack_2', 'stack_3', 'stack_4', 'stack_5',\n",
    "           'stack_6', 'stack_7', 'stack_8', 'stack_9', 'stack_10',\n",
    "           'stack_11', 'stack_12', 'stack_13', 'stack_14', 'stack_15',\n",
    "           'stack_16','stack_17', 'stack_18', 'stack_19', 'stack_20']]= stackaddresses_val_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "stackaddresses_test_enc = stack_ordinal_encoder.transform(X_cat_test[['stack_1', 'stack_2', 'stack_3', 'stack_4', 'stack_5',\n",
    "                                                        'stack_6', 'stack_7', 'stack_8', 'stack_9', 'stack_10',\n",
    "                                                        'stack_11', 'stack_12', 'stack_13', 'stack_14', 'stack_15',\n",
    "                                                        'stack_16','stack_17', 'stack_18', 'stack_19', 'stack_20']])\n",
    "\n",
    "stackaddresses_test_enc = np.where(stackaddresses_test_enc==-1, np.max(stackaddresses_train_enc)+1, stackaddresses_test_enc)\n",
    "\n",
    "X_cat_test[['stack_1', 'stack_2', 'stack_3', 'stack_4', 'stack_5',\n",
    "           'stack_6', 'stack_7', 'stack_8', 'stack_9', 'stack_10',\n",
    "           'stack_11', 'stack_12', 'stack_13', 'stack_14', 'stack_15',\n",
    "           'stack_16','stack_17', 'stack_18', 'stack_19', 'stack_20']]= stackaddresses_test_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Stack adresses\n",
    "args_ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1, dtype=float)\n",
    "args_train_enc = args_ordinal_encoder.fit_transform(X_cat_train[['name_1', 'name_2', 'name_3', 'name_4', 'name_5',\n",
    "                                                            'type_1', 'type_2', 'type_3', 'type_4', 'type_5',\n",
    "                                                            'value_1', 'value_2', 'value_3', 'value_4', 'value_5']])\n",
    "X_cat_train[['name_1', 'name_2', 'name_3', 'name_4', 'name_5',\n",
    "             'type_1', 'type_2', 'type_3', 'type_4', 'type_5',\n",
    "             'value_1', 'value_2', 'value_3', 'value_4', 'value_5']] = args_train_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_val_enc = args_ordinal_encoder.transform(X_cat_val[['name_1', 'name_2', 'name_3', 'name_4', 'name_5',\n",
    "                                                    'type_1', 'type_2', 'type_3', 'type_4', 'type_5',\n",
    "                                                    'value_1', 'value_2', 'value_3', 'value_4', 'value_5']])\n",
    "\n",
    "args_val_enc = np.where(args_val_enc==-1, np.max(args_train_enc)+1, args_val_enc)\n",
    "\n",
    "X_cat_val[['name_1', 'name_2', 'name_3', 'name_4', 'name_5',\n",
    "           'type_1', 'type_2', 'type_3', 'type_4', 'type_5',\n",
    "           'value_1', 'value_2', 'value_3', 'value_4', 'value_5']] = args_val_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_test_enc = args_ordinal_encoder.transform(X_cat_test[['name_1', 'name_2', 'name_3', 'name_4', 'name_5',\n",
    "                                                      'type_1', 'type_2', 'type_3', 'type_4', 'type_5',\n",
    "                                                      'value_1', 'value_2', 'value_3', 'value_4', 'value_5']])\n",
    "\n",
    "args_test_enc = np.where(args_test_enc==-1, np.max(args_train_enc)+1, args_test_enc)\n",
    "\n",
    "X_cat_test[['name_1', 'name_2', 'name_3', 'name_4', 'name_5',\n",
    "           'type_1', 'type_2', 'type_3', 'type_4', 'type_5',\n",
    "           'value_1', 'value_2', 'value_3', 'value_4', 'value_5']] = args_test_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ProcessName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1, dtype=float)\n",
    "X_cat_train['processName'] = proc_ordinal_encoder.fit_transform(X_cat_train[['processName']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "processName_val_enc = proc_ordinal_encoder.transform(X_cat_val[['processName']])\n",
    "processName_val_enc = np.where(processName_val_enc==-1, np.max(X_cat_train['processName'])+1, processName_val_enc)\n",
    "X_cat_val['processName'] = processName_val_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "processName_test_enc = proc_ordinal_encoder.transform(X_cat_test[['processName']])\n",
    "processName_test_enc = np.where(processName_test_enc==-1, np.max(X_cat_train['processName'])+1, processName_test_enc)\n",
    "X_cat_test['processName'] = processName_test_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HostName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "host_ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1, dtype=float)\n",
    "X_cat_train['hostName'] = host_ordinal_encoder.fit_transform(X_cat_train[['hostName']])\n",
    "\n",
    "host_val_enc = host_ordinal_encoder.transform(X_cat_val[['hostName']])\n",
    "host_val_enc = np.where(host_val_enc==-1, np.max(X_cat_train['hostName'])+1, host_val_enc)\n",
    "X_cat_val['hostName'] = host_val_enc\n",
    "\n",
    "host_test_enc = host_ordinal_encoder.transform(X_cat_test[['hostName']])\n",
    "host_test_enc = np.where(host_test_enc==-1, np.max(X_cat_train['hostName'])+1, host_test_enc)\n",
    "X_cat_test['hostName'] = host_test_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EventName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1, dtype=float)\n",
    "X_cat_train['eventName'] = event_ordinal_encoder.fit_transform(X_cat_train[['eventName']])\n",
    "\n",
    "event_val_enc = event_ordinal_encoder.transform(X_cat_val[['eventName']])\n",
    "event_val_enc = np.where(event_val_enc==-1, np.max(X_cat_train['eventName'])+1, event_val_enc)\n",
    "X_cat_val['eventName'] = event_val_enc\n",
    "\n",
    "event_test_enc = event_ordinal_encoder.transform(X_cat_test[['eventName']])\n",
    "event_test_enc = np.where(event_test_enc==-1, np.max(X_cat_train['eventName'])+1, event_test_enc)\n",
    "X_cat_test['eventName'] = event_test_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([X_num_train, X_cat_train], axis=1)\n",
    "y_train = train_data['sus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = pd.concat([X_num_val, X_cat_val], axis=1)\n",
    "y_val = val_data['sus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.concat([X_num_test, X_cat_test], axis=1)\n",
    "y_test = test_data['sus']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "188967"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our model processes a tensor of shape (batch size, sequence length, features), \n",
    "# where sequence length is the number of time steps and features is each input timeseries\n",
    "\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Attention and Normalization\n",
    "    x = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(inputs, inputs)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(res)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    return x + res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes=2\n",
    "\n",
    "def build_model(\n",
    "    input_shape,\n",
    "    head_size,\n",
    "    num_heads,\n",
    "    ff_dim,\n",
    "    num_transformer_blocks,\n",
    "    mlp_units,\n",
    "    dropout=0,\n",
    "    mlp_dropout=0,\n",
    "):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D(data_format=\"channels_last\")(x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "    outputs = layers.Dense(n_classes, activation=\"softmax\")(x)\n",
    "    return keras.Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "model = build_model(\n",
    "    input_shape,\n",
    "    head_size=256,\n",
    "    num_heads=4,\n",
    "    ff_dim=4,\n",
    "    num_transformer_blocks=4,\n",
    "    mlp_units=[128],\n",
    "    mlp_dropout=0.4,\n",
    "    dropout=0.25,\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    metrics=[\"sparse_categorical_accuracy\"],\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]\n",
    "\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=150,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "model.evaluate(x_test, y_test, verbose=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
